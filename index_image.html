<!DOCTYPE HTML>
<html lang="en"><head>

      <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4F2WV7NRX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-L4F2WV7NRX');
  </script>


  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zixuan Ke</title>

  <meta name="author" content="Zixuan Ke">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zixuan Ke</name>
              </p>
              <p>I am a 4th-year Ph.D. student at the <a href="https://www.cs.uic.edu/">University of Illinois, Chicago</a>, where I am advised by Prof. <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>.
                Prior to that, I received my M.Sc. in Computer Science from the <a href="https://www.cs.utdallas.edu/">University of Texas, Dallas</a> and was advised by Prof. <a href="https://www.hlt.utdallas.edu/~vince">Vincent Ng</a>.
                I have worked as a research intern at <a href="https://ai.facebook.com/">Meta AI</a>, <a href="https://www.amazon.science/">Amazon Science</a> and <a href="https://ai.tencent.com/ailab/nlp/en/index.html">Tencent AI Lab</a>.
                I am broadly interested in <strong>machine learning</strong> and <strong>natural language processing</strong>:
              </p>
              <p>
                In <strong>fundamental research</strong>, I work on <strong>continual and lifelong learning</strong>
                (NeurIPS<a href="https://proceedings.neurips.cc/paper/2020/hash/d7488039246a405baf6a7cbc3613a56f-Abstract.html">20</a>,
                <a href="https://proceedings.neurips.cc/paper/2021/file/bcd0049c35799cdf57d06eaf2eb3cff6-Paper.pdf">21</a>,
                <a href="https://arxiv.org/abs/2211.02633">22</a>)
              </p>
              <p>

               In <strong>applied research</strong>, I work on <strong>LLM pre-training</strong> (<a href="https://openreview.net/forum?id=m_GDIItaI3o">ICLR23</a>, <a href="https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.693.pdf">EMNLP22-1</a>, <a href="https://arxiv.org/abs/2210.05549">EMNLP22-2</a>)
                and various <strong>natural language processing</strong> tasks, such as
<!--                <strong>continual LM pre-training</strong> (<a href="https://openreview.net/forum?id=m_GDIItaI3o">ICLR23</a>, <a href="https://arxiv.org/abs/2210.05549">EMNLP22</a>),-->
                <strong>text classification</strong> (<a href="https://arxiv.org/pdf/2112.03271">NAACL21</a>, <a href="https://arxiv.org/pdf/2112.02714">EMNLP21</a>),
                and <strong>argument mining</strong> (ACL<a href="https://aclanthology.org/P18-1058/">18</a>,<a href="https://aclanthology.org/P19-1390/">19</a>; IJCAI<a href="https://www.ijcai.org/Proceedings/2018/0574.pdf">18</a>,<a href="https://www.ijcai.org/proceedings/2019/0879.pdf">19</a>).
              </p>
<!--              <p>-->
<!--                My long-term goal is to develop more general and intelligent systems, making the knowledge in the neural network more reusable and updatable in an ever-changing world.-->
<!--              </p>-->

              <p style="text-align:center">
                <a href="mailto:zke4@uic.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=SZ4sFNEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ZixuanKe">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/KeZixuan">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zixuanke_b.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zixuanke_b.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                 <li>(Jan., 2023) To appear in ICLR2023: <a href="https://openreview.net/forum?id=m_GDIItaI3o">Continual Learning of Language Models</a>.
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications &amp Preprints (full list in <a href="https://scholar.google.com/citations?user=SZ4sFNEAAAAJ&hl=en">Google Scholar</a>)</heading>
              <br> (*indicates equal contribution)
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
      <h3>Large Language Modeling</h3>

  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <img src='images/das.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
              <p>
                <a href="https://openreview.net/forum?id=m_GDIItaI3o">
                  <papertitle>Continual Learning of Language Models</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
                <summary>TL;DR</summary>
                <i>We study continual domain-adaptive pre-training. A novel soft-masking mechanism is proposed to not only preserve the pre-trained/general knowledge and previously learned domain knowledge, but also achieves knowledge transfer to improve end-task performances. Evaluation on 6 domains demonstrates the effectiveness. </i>
                </details>
                <br>
                  <strong>Zixuan Ke*</strong>,
                  <a href="https://shaoyijia.github.io/">Yijia Shao*</a>,
                  <a href="https://linhaowei1.github.io/">Haowei Lin*</a>,
                  Tatsuya Konishi,
                  <a href="https://k-gyuhak.github.io/">Gyuhak Kim</a>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>ICLR</em>, 2023
                <br>
                  <a href="https://arxiv.org/abs/2302.03241">arxiv</a>
              </p>
  </td>
</tr>
		  
<tr >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <img src='images/dga.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
              <p>
                <a href="https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.693.pdf">
                  <papertitle>Adapting a Language Model While Preserving its General Knowledge</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
                <summary>TL;DR</summary>
                <i>We argue that a good domain-adaptive pre-training has two needs: (1) the general knowledge should be preserved; and (2) the LM should be specialized to the target domain due to polysemy. We then propose a novel informed adaptation method and show its effectiveness in 6 different domains. </i>
                </details>
                <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://shaoyijia.github.io/">Yijia Shao</a>,
                  <a href="https://linhaowei1.github.io/">Haowei Lin</a>,
                  <a href="https://howardhsu.github.io/">Hu Xu</a>,
                  <a href="https://leishu02.github.io/">Lei Shu</a>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>EMNLP</em>, 2022
                <br>
                  <a href="https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.693.pdf">arxiv</a> /
                  <a href="https://vincent950129.github.io/data/dga.pdf">poster</a> /
                  <a href="https://github.com/UIC-Liu-Lab/DGA">code</a>
              </p>
  </td>
</tr>
          
<tr >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <img src='images/dga.jpg' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
                <p>
                <a href="https://arxiv.org/abs/2210.05549">
                  <papertitle>Continual Training of Language Models for Few-Shot Learning</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
                <summary>TL;DR</summary>
                <i>We propose the problem of continual domain-adaptive pre-training and compare its differences and challenges with conventional continual learning. A novel task masking method is proposed and is effective in 4 different domains. </i>
                </details>
                <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://linhaowei1.github.io/">Haowei Lin</a>,
                  <a href="https://shaoyijia.github.io/">Yijia Shao</a>,
                  <a href="https://howardhsu.github.io/">Hu Xu</a>,
                  <a href="https://leishu02.github.io/">Lei Shu</a>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>EMNLP</em>, 2022
                <br>
                  <a href="https://arxiv.org/abs/2210.05549">arxiv</a> /
                  <a href="https://vincent950129.github.io/data/cpt.pdf">poster</a> /
                  <a href="https://github.com/UIC-Liu-Lab/CPT">code</a>
              </p>
  </td>
</tr>

					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
