<!DOCTYPE HTML>
<html lang="en"><head>

    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-L4F2WV7NRX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-L4F2WV7NRX');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zixuan Ke</title>

  <meta name="author" content="Zixuan Ke">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zixuan Ke</name>
              </p>
              <p>
                I earned the Ph.D. degree at the <a href="https://www.cs.uic.edu/">University of Illinois, Chicago</a>,
                where I was fortunate to be advised by <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a> (we continue to work closely).
<!--                under the guidance of Professor <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>.-->
                Prior to that, I received my M.Sc. in Computer Science from the <a href="https://www.cs.utdallas.edu/">University of Texas, Dallas</a>,
                under the guidance of <a href="https://www.hlt.utdallas.edu/~vince">Vincent Ng</a>.
<!--                where Professor <a href="https://www.hlt.utdallas.edu/~vince">Vincent Ng</a> served as my advisor.-->
                During the summers, I was a research intern at <a href="https://research.google/teams/athena/">Google Research</a>, <a href="https://ai.facebook.com/">Meta AI</a>, and <a href="https://www.amazon.science/">Amazon Science</a>.
<!--                and <a href="https://ai.tencent.com/ailab/nlp/en/index.html">Tencent AI Lab</a>.-->
              </p>
              <p>
<!--                  My research studies the reusability and updatability of foundation models, particularly language models.-->
                   My research studies how to adapt the <strong>foundation models</strong>, particularly <strong>large language models</strong> (LLMs), for an <strong>ever-changing world</strong> characterized by <strong>emerging</strong> domains, events, topics or information.
<!--                  and make their knowledge more <strong>reusable</strong> and <strong>updatable</strong>.-->
<!--                  This involves various areas, including <strong>machine learning</strong>, <strong>large language model</strong> and <strong>natural language processing</strong>.-->
<!--                (a <a href="profile/map.png">research topic map</a>).-->
              </p>
              <p>
                This includes (but is not limited to)!
              <ul>
                <li><strong>Large Language Model (pre-training, post-training and frontiers, e.g., retrieval-augmented generation) </strong></li>
                <ul>
                  <li><a href="https://arxiv.org/abs/2401.06954">arXiv24</a>, <a href="https://openreview.net/forum?id=m_GDIItaI3o">ICLR23</a>, <a href="https://arxiv.org/abs/2301.08986">EMNLP22a</a>, <a href="https://arxiv.org/abs/2210.05549">EMNLP22b</a></li>
              </ul>
                  <li><strong>Continual and Lifelong Learning (of tasks, classes, and domains)</strong>
              <ul>
                  <li><a href="https://arxiv.org/abs/2306.14775">ICML23</a>, <a href="https://arxiv.org/abs/2211.12701">arXiv23</a>, NeurIPS<a href="https://proceedings.neurips.cc/paper/2020/hash/d7488039246a405baf6a7cbc3613a56f-Abstract.html">20</a>,<a href="https://proceedings.neurips.cc/paper/2021/file/bcd0049c35799cdf57d06eaf2eb3cff6-Paper.pdf">21</a>,<a href="https://arxiv.org/abs/2211.02633">22</a></li>
              </ul>
                   <li><strong>Natural Language Processing (classification, generation and extraction)</strong>
              <ul>
                  <li><a href="https://arxiv.org/abs/2310.09436">EMNLP23</a>, <a href="https://arxiv.org/pdf/2112.03271">NAACL21</a>, <a href="https://arxiv.org/pdf/2112.02714">EMNLP21</a></li>
               </ul>
                <li><strong>Argument Mining</strong>
              <ul>
                <li>ACL<a href="https://aclanthology.org/P18-1058/">18</a>,<a href="https://aclanthology.org/P19-1390/">19</a>; IJCAI<a href="https://www.ijcai.org/Proceedings/2018/0574.pdf">18</a>,<a href="https://www.ijcai.org/proceedings/2019/0879.pdf">19</a></li>
              </ul>

              </ul>
              </p>

<!--              <p>-->
<!--                In terms of <strong>fundamental research</strong>, my work is centered around <strong>continual and lifelong learning</strong> (<a href="https://arxiv.org/abs/2306.14775">ICML23</a>, NeurIPS<a href="https://proceedings.neurips.cc/paper/2020/hash/d7488039246a405baf6a7cbc3613a56f-Abstract.html">20</a>,-->
<!--                <a href="https://proceedings.neurips.cc/paper/2021/file/bcd0049c35799cdf57d06eaf2eb3cff6-Paper.pdf">21</a>,-->
<!--                <a href="https://arxiv.org/abs/2211.02633">22</a>).-->
<!--              </p>-->
              <p>
<!--                In the realm of <strong>applied research</strong>, I focus on  <strong>large language model</strong> (<a href="https://openreview.net/forum?id=m_GDIItaI3o">ICLR23</a>, <a href="https://arxiv.org/abs/2301.08986">EMNLP22a</a>, <a href="https://arxiv.org/abs/2210.05549">EMNLP22b</a>).-->
<!--                I also work on various <strong>natural language processing</strong> tasks (<a href="https://arxiv.org/abs/2310.09436">EMNLP23</a>, <a href="https://arxiv.org/pdf/2112.03271">NAACL21</a>, <a href="https://arxiv.org/pdf/2112.02714">EMNLP21</a>),-->
<!--                and <strong>argument mining</strong> (ACL<a href="https://aclanthology.org/P18-1058/">18</a>,<a href="https://aclanthology.org/P19-1390/">19</a>; IJCAI<a href="https://www.ijcai.org/Proceedings/2018/0574.pdf">18</a>,<a href="https://www.ijcai.org/proceedings/2019/0879.pdf">19</a>).-->
              </p>
<!--              such as <strong>text classification</strong>-->
<!--              <p>-->
<!--                My long-term goal is to develop more general and intelligent systems, making the knowledge in the neural network more reusable and updatable in an ever-changing world.-->
<!--              </p>-->

              <p style="text-align:center">
                <a href="mailto:zke4@uic.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=SZ4sFNEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ZixuanKe">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/KeZixuan">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zixuan-ke-b23477292/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://zixuanke.substack.com/">Blog</a>
<!--                  &nbsp/&nbsp-->
<!--                <a href="https://vincent950129.github.io/pdf/ZixuanCV.pdf">Resume</a>-->
                <p>
                If you'd like to chat with me about research or anything,
                  please feel free to reach out via email or schedule a chat <a href="https://calendly.com/zke4/20min">here</a>.
              </p>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="profile/zixuanke_b.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="profile/zixuanke_b.jpg" class="hoverZoomLink"></a>
<!--              <figcaption>Vincent (left) and me (right)</figcaption>-->
            </td>
          </tr>
        </tbody></table>

<!--      <p>-->
<!--        <u><strong>I am now on the industry job markets.-->
<!--          If you think my experience would be a good fit for your organization or institution,-->
<!--          please feel free to reach out!</strong></u>-->
<!--      </p>-->

<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: -30px; margin-top: -40px;"><tbody>-->
<!--          <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>News</heading>-->
<!--              <ul>-->
<!--                <li><strong><font size="3">  I am now on the job market!-->
<!--                      If you think my experience would be a good fit for your organization or institution,-->
<!--                      please feel free to reach out!</font> </strong></li>-->
<!--&lt;!&ndash;                <li>(Oct., 2023) To appear in EMNLP2023 (findings): Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks</a>.</li>&ndash;&gt;-->
<!--&lt;!&ndash;                <li>(Jan., 2023) To appear in ICLR2023: <a href="https://openreview.net/forum?id=m_GDIItaI3o">Continual Pre-training of Language Models</a>. See you in <a href="https://iclr.cc/Conferences/2023/VisitKigali">Kigali, Rwanda</a>.</li>&ndash;&gt;-->
<!--&lt;!&ndash;                 <li>(Nov., 2022) New preprint: <a href="https://arxiv.org/abs/2211.12701">Continual Learning of Natural Language Processing Tasks: A Survey</a>.&ndash;&gt;-->
<!--&lt;!&ndash;                 <li>(August, 2022) New preprint: <a href="https://arxiv.org/abs/2208.05516">Quality not quantity: on robustness and dataset design</a> (led by <a href="https://thaonguyen19.github.io/">Thao Nguyen</a>).&ndash;&gt;-->
<!--&lt;!&ndash;								 <li>(May, 2022) New preprint: <a href="https://arxiv.org/abs/2205.01397">Data determines distributional robustness</a> (led by Alex Fang).&ndash;&gt;-->
<!--&lt;!&ndash;								 <li>(March, 2022) New preprint: <a href="https://arxiv.org/abs/2203.10421">CLIP on Wheels (CoW)</a> (led by <a href="https://sagadre.github.io/">Samir Gadre</a>).&ndash;&gt;-->
<!--&lt;!&ndash;								 <li>(March, 2022) New preprint: <a href="https://arxiv.org/abs/2203.05482">Model soups</a>.&ndash;&gt;-->
<!--              </ul>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications &amp Preprints (full list in <a href="https://scholar.google.com/citations?user=SZ4sFNEAAAAJ&hl=en">Google Scholar</a>)</heading>
              <br> (*indicates equal contribution)


<!--              <h3>Preprint</h3>-->
<!--                <a href="https://arxiv.org/abs/2211.12701">-->
<!--                  <papertitle>Continual Learning of Natural Language Processing Tasks: A Survey</papertitle>-->
<!--                </a>-->
<!--                <br>-->
<!--                  <strong>Zixuan Ke</strong>,-->
<!--                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>-->
<!--                <br>-->
<!--                <em>Arkiv</em>, 2022-->
<!--                <br>-->

              <h3>Large Language Model</h3>
              <p>
                <a href="https://arxiv.org/abs/2401.06954">
                  <papertitle>Bridging the Preference Gap between Retrievers and LLMs</papertitle>
                </a>
              <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://sites.google.com/site/weizekong/home">Weize Kong</a>,
                  <a href="https://research.google/people/cheng-li/">Cheng Li</a>,
                  <a href="https://research.google/people/mingyang-zhang/">Mingyang Zhang</a>,
                  <a href="https://public.websites.umich.edu/~qmei/">Qiaozhu Mei</a>,
                  <a href="https://bendersky.github.io/">Michael Bendersky</a>
                <br>
                <em>arXiv</em>, 2024
                <br>
                  <a href="https://arxiv.org/abs/2401.06954">arxiv</a>
              </p>

                <p>
                <a href="https://openreview.net/forum?id=m_GDIItaI3o">
                  <papertitle>Continual Pre-training of Language Models</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
                <summary>TL;DR</summary>
                <i>Our study examines the continual Pre-training of language models (LMs) in various settings, with a specific focus on continual domain-adaptive pre-training of LMs. To preserve pre-trained/general knowledge and domain knowledge, we propose a novel soft-masking mechanism that also enables knowledge transfer, thus improving end-task performances. Results from evaluations conducted on 6 different domains demonstrate the effectiveness of this approach.
                </i>
                </details>
<!--                <details style="margin-bottom: -15px;">-->
<!--                <summary>Illustration</summary>-->
<!--                <img src='profile/das.jpg' width="500">-->
<!--                </details>-->

                <br>
                  <strong>Zixuan Ke*</strong>,
                  <a href="https://shaoyijia.github.io/">Yijia Shao*</a>,
                  <a href="https://linhaowei1.github.io/">Haowei Lin*</a>,
                  <a href="https://scholar.google.com/citations?user=tx15SxoAAAAJ&hl">Tatsuya Konishi</a>,
                  <a href="https://k-gyuhak.github.io/">Gyuhak Kim</a>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>ICLR</em>, 2023
                <br>
                  <a href="https://arxiv.org/abs/2302.03241">arxiv</a> /
                  <a href="https://vincent950129.github.io/pdf/das_poster.pdf">poster </a>/
                  <a href="https://huggingface.co/UIC-Liu-Lab/DAS-Rest2Cam">model-card </a>/
                  <a href="https://github.com/UIC-Liu-Lab/ContinualLM">code</a>
                  <iframe src="https://ghbtns.com/github-btn.html?user=UIC-Liu-Lab&repo=ContinualLM&type=star&count=true" frameborder="0" scrolling="0" width="110" height="20" title="GitHub" align="bottom" style="position:absolute;Left:30;bottom:-10"></iframe>

              </p>
              <p>
                <a href="https://arxiv.org/abs/2301.08986">
                  <papertitle>Adapting a Language Model While Preserving its General Knowledge</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
                <summary>TL;DR</summary>
                <i>Our argument is that an effective method for domain-adaptive pre-training of language models (LMs) should satisfy two requirements: (1) the preservation of general knowledge, and (2) the specialization of the LM to the target domain due to polysemy. To address these needs, we propose a novel informed adaptation method, which we evaluate across 6 different domains and demonstrate its effectiveness.
                </i>
                </details>
<!--                <details style="margin-bottom: -15px;">-->
<!--                <summary>Illustration</summary>-->
<!--                <img src='profile/dga.jpg' width="300">-->
<!--                </details>-->
                <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://shaoyijia.github.io/">Yijia Shao</a>,
                  <a href="https://linhaowei1.github.io/">Haowei Lin</a>,
                  <a href="https://howardhsu.github.io/">Hu Xu</a>,
                  <a href="https://leishu02.github.io/">Lei Shu</a>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>EMNLP</em>, 2022a
                <br>
                  <a href="https://arxiv.org/abs/2301.08986">arxiv</a> /
                  <a href="https://vincent950129.github.io/pdf/dga.pdf">poster</a> /
                  <a href="https://github.com/UIC-Liu-Lab/DGA">code</a>
              </p>
                <p>
                <a href="https://arxiv.org/abs/2210.05549">
                  <papertitle>Continual Training of Language Models for Few-Shot Learning</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
               <summary>TL;DR</summary>
                <i>Our proposal concerns the challenge of continual domain-adaptive pre-training of language models (LMs) and its differences and challenges compared to conventional continual end-task learning. To address this challenge, we propose a novel task masking method, which we evaluate across 4 different domains and find it to be effective.
                </i>
                </details>
<!--                <details style="margin-bottom: -15px;">-->
<!--                <summary>Illustration</summary>-->
<!--                <img src='profile/cpt.jpg' width="300">-->
<!--                </details>-->
                <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://linhaowei1.github.io/">Haowei Lin</a>,
                  <a href="https://shaoyijia.github.io/">Yijia Shao</a>,
                  <a href="https://howardhsu.github.io/">Hu Xu</a>,
                  <a href="https://leishu02.github.io/">Lei Shu</a>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>EMNLP</em>, 2022b
                <br>
                  <a href="https://arxiv.org/abs/2210.05549">arxiv</a> /
                  <a href="https://vincent950129.github.io/pdf/cpt.pdf">poster</a> /
                  <a href="https://huggingface.co/UIC-Liu-Lab/CPT">model-card </a>/
                  <a href="https://github.com/UIC-Liu-Lab/CPT">code</a>
              </p>
              <h3>Continual Learning</h3>
              <p>
                <a href="https://arxiv.org/abs/2310.09436">
                  <papertitle>Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks</papertitle>
                </a>
<!--                <details style="margin-bottom: -15px; margin-top: -15px;">-->
<!--                <summary>TL;DR</summary>-->
<!--                <i>Many existing continual learning methods focus solely on mitigating forgetting, without a mechanism for promoting knowledge transfer. Our proposal is a capsule-based method that addresses both challenges. We evaluate the effectiveness of our approach across 4 different datasets.-->
<!--                </i>-->
<!--                </details >-->
<!--                <details style="margin-bottom: -15px;">-->
<!--                <summary>Illustration</summary>-->
<!--                <img src='profile/ctr.jpg' width="500">-->
<!--                </details>-->
              <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>,
                  <a href="https://xwhan.github.io/">Wenhan Xiong</a>,
                  <a href="http://asli.us/">Asli Celikyilmaz</a>,
                  <a href="https://ai.meta.com/people/haoran-li/">Haoran Li</a>
                <br>
                <em>EMNLP</em>, 2023
                <br>
                  <a href="https://arxiv.org/abs/2310.09436">arxiv</a> /
<!--                  <a href="https://slideslive.com/38969013">talk</a> /-->
<!--                  <a href="https://vincent950129.github.io/pdf/ctr.pdf">poster</a> /-->
                  <a href="https://github.com/ZixuanKe/PyContinual">code</a>
                  <iframe src="https://ghbtns.com/github-btn.html?user=zixuanke&repo=PyContinual&type=star&count=true" frameborder="0" scrolling="0" width="110" height="20" title="GitHub" align="bottom" style="position:absolute;Left:30;bottom:-10"></iframe>
<!--                  <iframe src="https://ghbtns.com/github-btn.html?user=zixuanke&repo=PyContinual&type=fork&count=true" frameborder="0" scrolling="0" width="110" height="20" title="GitHub" align="bottom" style="float:right;Left:30;bottom:-10"></iframe>-->

              </p>

                <a href="https://arxiv.org/abs/2211.12701">
                  <papertitle>Continual Learning of Natural Language Processing Tasks: A Survey</papertitle>
                </a>
              <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>arXiv</em>, 2023
                <br>
                  <a href="https://arxiv.org/abs/2211.12701">arxiv</a>
              </p>

              <p>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/20f44da80080d76bbc35bca0027f14e6-Paper-Conference.pdf">
                  <papertitle>A Theoretical Study on Solving Continual Learning</papertitle>
                </a>
              <br>
                  <a href="https://k-gyuhak.github.io/">Gyuhak Kim</a>,
                  <a href="https://scholar.google.com/citations?user=YVW6nPoAAAAJ&hl=en">Changnan Xiao</a>,
                  <strong>Zixuan Ke</strong>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>
                <br>
                <em>NeurIPS</em>, 2022
                <br>
                  <a href="https://arxiv.org/abs/2211.02633">arxiv</a>
              </p>

              <p>
                <a href="https://proceedings.neurips.cc/paper/2021/file/bcd0049c35799cdf57d06eaf2eb3cff6-Paper.pdf">
                  <papertitle>Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
                <summary>TL;DR</summary>
                <i>Many existing continual learning methods focus solely on mitigating forgetting, without a mechanism for promoting knowledge transfer. Our proposal is a capsule-based method that addresses both challenges. We evaluate the effectiveness of our approach across 4 different datasets.
                </i>
                </details >
<!--                <details style="margin-bottom: -15px;">-->
<!--                <summary>Illustration</summary>-->
<!--                <img src='profile/ctr.jpg' width="500">-->
                </details>
              <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>,
                  <a href="https://www.linkedin.com/in/nianzu-ma-77b56350">Nianzu Ma</a>,
                  <a href="https://howardhsu.github.io/">Hu Xu</a>,
                  <a href="https://leishu02.github.io/">Lei Shu</a>
                <br>
                <em>NeurIPS</em>, 2021
                <br>
                  <a href="https://arxiv.org/abs/2112.02706">arxiv</a> /
                  <a href="https://slideslive.com/38969013">talk</a> /
                  <a href="https://vincent950129.github.io/pdf/ctr.pdf">poster</a> /
                  <a href="https://github.com/ZixuanKe/PyContinual">code</a>
                  <iframe src="https://ghbtns.com/github-btn.html?user=zixuanke&repo=PyContinual&type=star&count=true" frameborder="0" scrolling="0" width="110" height="20" title="GitHub" align="bottom" style="position:absolute;Left:30;bottom:-10"></iframe>
<!--                  <iframe src="https://ghbtns.com/github-btn.html?user=zixuanke&repo=PyContinual&type=fork&count=true" frameborder="0" scrolling="0" width="110" height="20" title="GitHub" align="bottom" style="float:right;Left:30;bottom:-10"></iframe>-->

              </p>

              <p>
                <a href="https://proceedings.neurips.cc/paper/2020/hash/d7488039246a405baf6a7cbc3613a56f-Abstract.html">
                  <papertitle>Continual Learning of A Mixed Sequence of Similar and Dissimilar Tasks</papertitle>
                </a>
                <details style="margin-bottom: -15px; margin-top: -15px;">
              <summary>TL;DR</summary>
                <i>Existing research on continual learning focused on dealing with forgetting, where the tasks are assumed to be dissimilar and have little shared knowledge. No technique has been proposed to learn a sequence of mixed similar and dissimilar tasks that can deal with forgetting and also transfer knowledge forward and backward. This paper proposes such a technique and empirical evaluation using sequences of mixed tasks demonstrates the effectiveness of the proposed model. </i>
                </details>
<!--                <details style="margin-bottom: -15px;">-->
<!--                <summary>Illustration</summary>-->
<!--                <img src='profile/cat.jpg' width="500">-->
<!--                </details>-->
                <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>,
                  <a href="https://people.mpi-inf.mpg.de/~xhuang/">Xingchang Huang</a>
                <br>
                <em>NeurIPS</em>, 2020
                <br>
                  <a href="https://arxiv.org/abs/2112.10017">arxiv</a> /
                  <a href="https://slideslive.com/38937183">talk</a> /
                  <a href="https://vincent950129.github.io/pdf/cat.pdf">poster</a> /
                  <a href="https://github.com/ZixuanKe/CAT">code</a>

              </p>

              <h3>Argument Mining</h3>
              <p>
                <a href="https://www.ijcai.org/proceedings/2019/0879.pdf">
                  <papertitle>Automated Essay Scoring: A Survey of the State of the Art</papertitle>
                </a>
                <br>
                  <strong>Zixuan Ke</strong>,
                  <a href="https://www.hlt.utdallas.edu/~vince">Vincent Ng</a>
                <br>
                <em>IJCAI</em>, 2019
              </p>
              <p>
                <a href="https://www.ijcai.org/Proceedings/2018/0574.pdf">
                  <papertitle>Learning to Give Feedback: Modeling Attributes Affecting Argument Persuasiveness in Student Essays</papertitle>
                </a>
                <br>
                  <strong>Zixuan Ke</strong>, Winston Carlile, Nishant Gurrapadi, 
                  <a href="https://www.hlt.utdallas.edu/~vince">Vincent Ng</a>
                <br>
                <em>IJCAI</em>, 2018
                <br>
                  <a href="https://www.hlt.utdallas.edu/~zixuan/EssayScoring/">dataset</a> /
                  <a href="https://www.hlt.utdallas.edu/~zixuan/EssayScoring/persuasiveness/">dataset-persuasive</a> /
                  <a href="https://www.hlt.utdallas.edu/~zixuan/EssayScoring/ThesisStrength/">dataset-thesis-strength</a>
              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: -30px; margin-top: -40px;"><tbody>
          <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent Talks &amp Classes</heading>
                <p>
              <ul>
                <li> Adapting Large Language Models for the Dynamic World (<a href="https://vincent950129.github.io/pdf/zixuan_talk.pdf">slides</a>)</li>
                <ul>
                  <li> Talk at <a href="https://www.snowflake.com/">Snowflake</a>, Remote, Feb 1, 2024. </li>
                  <li> Talk at <a href="https://www.salesforceairesearch.com/">Salesforce AI Research</a>, Remote, Jan 11, 2024. </li>
                  <li> Talk at <a href="https://research.google/teams/athena/">Google Research</a>, Remote, Nov 9, 2023. </li>
                </ul>
            </ul>

                <ul><li> Continual Pre-training of Language Models (<a href="https://vincent950129.github.io/pdf/continualAI.pdf">slides</a>, <a href="https://youtu.be/FmfRukpRKjg?t=2431">video</a>), Talk at <a href="https://www.continualai.org/activities/2023-seminar-series">ContinualAI</a>, Remote, April 27, 2023.  </li></ul>
              <ul><li> Continual Learning in NLP (<a href="https://vincent950129.github.io/pdf/DEIM-2023-Zixuan.pdf">slides</a>), Tutorial at <a href="https://event.dbsj.org/deim2023/post/tutorial.html">DEIM23</a>, Remote, March 6, 2023.  </li></ul>
<!--              (<a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>, <strong>Zixuan Ke</strong>, <a href="https://k-gyuhak.github.io/">Gyuhak Kim</a> and <a href="https://scholar.google.com/citations?user=tx15SxoAAAAJ&hl">Tatsuya Konishi</a>) -->
              <ul><li> Lifelong and Continual Learning (<a href="https://www.cs.uic.edu/~liub/Part-1-continual-learning-slides.pdf">Part 1</a>, <a href="https://vincent950129.github.io/pdf/Part-2-continual-learning-slides.pdf">Part 2</a>). A Short PhD Course (8 hours), Aalborg University, June 14-16, 2022.</li></ul>
<!--                (<a href="https://www.cs.uic.edu/~liub/">Bing Liu</a> and <strong>Zixuan Ke</strong>)  -->
              <ul><li> Press Coverage: <a href="https://xtech.nikkei.com/atcl/nxt/mag/rob/18/012600001/00132/">Nikkei Robotics</a> </li></ul>
              <ul><li> Conference talks (please refer to the <u><i>Selected Publications</i></u> section, and you can find more <a href="https://underline.io/speakers/97701-zixuan-ke">here</a>) </li></ul>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: -30px; margin-top: -40px;"><tbody>
          <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Services</heading>

                <p>
                  <ul><li>
                  Program Committee/Reviewer (since 2021):
                    <ul><li> ICLR, NeurIPS, ICML, ACL, EMNLP, NAACL, IJCAI, ARR, COLING, Collas, NLPCC </li></ul>
                  </li></ul>
                </p>
                <p>
                  <ul><li>
                  Journal Reviewer (since 2021):
                    <ul><li> TPAMI, TKDE, Neural Networks, Neurocomputing, Artificial Intelligence,  TALLIP </li></ul>
                  </li></ul>
                </p>
<!--                <p>-->
<!--                  <ul><li>-->
<!--                  Previously, I spent a great year at the Allen Institute for AI with <a href="https://roozbehm.info/index.html">Roozbeh Mottaghi</a>, and interned at-->
<!--                  Apple with <a href="https://mrastegari.github.io/">Mohammad Rastegari</a>.-->
<!--                  </li></ul>-->
<!--                </p>-->
<!--                <p>-->
<!--                  <ul><li>-->
<!--                  I completed my undergraduate thesis in Applied Mathematics at Brown University with senior thesis advisor <a href="https://www.brown.edu/academics/applied-mathematics/faculty/kavita-ramanan/home">Professor Kavita Ramanan</a>.-->
<!--                  Thesis:-->
<!--                  <a href="https://mitchellnw.github.io/pdf/brown_thesis.pdf">Interacting Particles Systems and Efficient Approximations for Large Sparse Graphs</a>.-->
<!--                  This was my first research experience, for which I am grateful.-->
<!--                  </li></ul>-->
<!--                </p>-->
<!--                <p>-->
<!--                  <ul><li>-->
<!--                  I am from Toronto, Canada and enjoy music, skiing, hiking, camping, reading, and climbing.-->
<!--                  </li></ul>-->
<!--                </p>-->

<!--              Teaching: Pre-doctoral student mentoring-->

<!--              Xinyan (Velocity) Yu, BS->MS at UW (2022‚ÄìCurrent)-->

            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: -30px; margin-top: -40px;"><tbody>
          <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards</heading>
              <br>
                <p>
                  <ul><li> Exceptional Research Premise (the highest honor for CoE PhD students at UIC), 2023
              </li></ul>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom: -30px; margin-top: -40px;"><tbody>
          <tr onmouseout="end_user_stop()" onmouseover="end_user_start()">
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Collaborators</heading>
              <br>
              I have had the privilege of working with and learning from great mentors and mentees, including:
                <p>
                  <ul><li>
                 Mentors:
                    <ul><li> <a href="https://www.cs.uic.edu/~liub/">Bing Liu</a>, distinguished professor at UIC</li></ul>
                    <ul><li> <a href="https://www.hlt.utdallas.edu/~vince">Vincent Ng</a>, professor at UTD </li></ul>
                    <ul><li> <a href="https://howardhsu.github.io/">Hu Xu</a>, research scientist at Facebook AI Research (FAIR) </li></ul>
                    <ul><li> <a href="https://leishu02.github.io/">Lei Shu</a>, research scientist at Google Research </li></ul>
              </li></ul>
              </p>
                <p>
                  <ul><li>
<!--              They're actually achieving more than I am! And I couldn't be more thrilled and proud of them!-->
                  Mentees:
                  <br>
                  (They're making great achievements and I couldn't be more thrilled and proud of them)
                    <ul><li><a href="https://shaoyijia.github.io/">Yijia Shao</a>, BS at Peking University ->PhD at Standford</li></ul>
                    <ul><li><a href="https://linhaowei1.github.io/">Haowei Lin</a>, BS->PhD at Peking University </li></ul>
                  </li></ul>
                </p>

<!--                Peers-->
            </td>
          </tr>
        </tbody></table>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <br>
              <p style="text-align:right;font-size:small;">
                Template modified from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
